#!/usr/bin/env python
# coding: utf-8

# # Question 1.

# In[4]:


def parse_sam_row(row, headers):
    # Split the row into fields using TAB as the delimiter
    fields = row.strip().split('\t')

    # Create a dictionary mapping headers to their corresponding values
    sam_dict = {header: fields[i] for i, header in enumerate(headers)}

    return sam_dict

def write_dict_to_txt(dictionary, output_file):
    with open(output_file, 'w') as file:
        for key, value in dictionary.items():
            file.write(f"{key}: {value}\n")

# Example usage with the provided test data
test_row = "HWI-ST397:389:C42DEACXX:8:1101:1161:8228	0	7696	1149	60	29S72M	*	0	0	GAGGTNNNNNNNNNNNNNNNNNNNNNNNNAAAGNATGATCAGGGAGTGGAACCTTAGCTCGAGCCCGCAGATGACCTGGATGGAGGCACGGAGGAGCAGGG	<<<?@########################-0<=#0<=?????????=???????????@@??===<<<<<<====<=<<<==<=<<<<<::6:95<<8<<6	NM:i:2	MD:Z:4T20G46	AS:i:65	XS:i:0"
headers = ["QNAME", "FLAG", "RNAME", "POS", "MAPQ", "CIGAR", "RNEXT", "PNEXT", "TLEN", "SEQ", "QUAL"]

result = parse_sam_row(test_row, headers)

# Specify the output file name
output_file_name = "parsed_sam_result.txt"

# Write the result to the specified text file
write_dict_to_txt(result, output_file_name)

print(f"Result has been written to {output_file_name}")


# In[7]:


import pandas as pd

def parse_sam(fname, headers, int_cols):
    # Read all lines from the SAM file into a list
    with open(fname, 'r') as file:
        sam_lines = file.readlines()

    # Initialize an empty list to store dictionaries
    sam_dicts = []

    # Loop through all lines and parse each line into a dictionary
    for line in sam_lines:
        # Check if the line is from the alignment section (does not start with '@')
        if not line.startswith('@'):
            sam_dict = parse_sam_row(line, headers)

            # Convert specified columns to integers
            for col in int_cols:
                # Check if the column is present in sam_dict before converting
                if col in sam_dict:
                    sam_dict[col] = int(sam_dict[col])

            # Append the dictionary to the list
            sam_dicts.append(sam_dict)

    # Create a DataFrame from the list of dictionaries
    sam_df = pd.DataFrame(sam_dicts)

    return sam_df

# Example usage with the provided test data
sam_file_path = "C:\\Users\\Ngugi\\Downloads\\Clone_Seq_Aligned.sam"
headers = ["QNAME", "FLAG", "RNAME", "POS", "MAPQ", "CIGAR", "RNEXT", "PNEXT", "TLEN", "SEQ", "QUAL"]
int_columns = ["POS", "MAPQ", "PNEXT", "TLEN", "AS", "XS"]

# Call the function to parse the SAM file and create a DataFrame
parsed_sam_df = parse_sam(sam_file_path, headers, int_columns)

# Specify the output file name
output_file_name = "parsed_sam_result.txt"

# Write the DataFrame to the specified text file
parsed_sam_df.to_csv(output_file_name, sep='\t', index=False)

print(f"DataFrame has been written to {output_file_name}")


# In[8]:


import pandas as pd

def parse_sam(fname, headers, int_cols):
    # Read all lines from the SAM file into a list
    with open(fname, 'r') as file:
        sam_lines = file.readlines()

    # Initialize an empty list to store dictionaries
    sam_dicts = []

    # Loop through all lines and parse each line into a dictionary
    for line in sam_lines:
        # Check if the line is from the alignment section (does not start with '@')
        if not line.startswith('@'):
            sam_dict = parse_sam_row(line, headers)

            # Convert specified columns to integers
            for col in int_cols:
                # Check if the column is present in sam_dict before converting
                if col in sam_dict:
                    sam_dict[col] = int(sam_dict[col])

            # Append the dictionary to the list
            sam_dicts.append(sam_dict)

    # Create a DataFrame from the list of dictionaries
    sam_df = pd.DataFrame(sam_dicts)

    return sam_df

# Example usage with the provided test data
sam_file_path = "C:\\Users\\Ngugi\\Downloads\\Clone_Seq_Aligned.sam"
headers = ["QNAME", "FLAG", "RNAME", "POS", "MAPQ", "CIGAR", "RNEXT", "PNEXT", "TLEN", "SEQ", "QUAL"]
int_columns = ["POS", "MAPQ", "PNEXT", "TLEN", "AS", "XS"]

# Call the function to parse the SAM file and create a DataFrame
parsed_sam_df = parse_sam(sam_file_path, headers, int_columns)

# Specify the output CSV file name
output_csv_file = "parsed_sam_result.csv"

# Write the DataFrame to a CSV file
parsed_sam_df.to_csv(output_csv_file, index=False)

print(f"Result has been written to {output_csv_file}")


# In[10]:


import pandas as pd

# Assuming you already have the parse_sam function defined

# Specify the path to the SAM file
sam_file_path = "C:\\Users\\Ngugi\\Downloads\\Clone_Seq_Aligned.sam"

# Define the headers and integer columns
headers = ["QNAME", "FLAG", "RNAME", "POS", "MAPQ", "CIGAR", "RNEXT", "PNEXT", "TLEN", "SEQ", "QUAL"]
int_columns = ["FLAG", "POS", "MAPQ", "PNEXT", "TLEN"]

# Use the parse_sam function to load the SAM file into a DataFrame
df_clone_seq = parse_sam(sam_file_path, headers, int_columns)

# Additional processing steps
# Remove unmapped reads (reads with RNAME '*')
df_clone_seq = df_clone_seq[df_clone_seq['RNAME'] != '*']

# Keep only reads with FLAG=0
df_clone_seq = df_clone_seq[df_clone_seq['FLAG'] == 0]

# Specify the output CSV file name
output_csv_file = "processed_clone_seq.csv"

# Write the DataFrame to a CSV file
df_clone_seq.to_csv(output_csv_file, index=False)

print(f"Processed DataFrame has been written to {output_csv_file}")

# Specify the output TXT file name
output_txt_file = "processed_clone_seq.txt"

# Write the DataFrame to a TXT file
df_clone_seq.to_string(output_txt_file, index=False)

print(f"Processed DataFrame has been written to {output_txt_file}")


# In[11]:


# Assuming df_clone_seq is the processed DataFrame

# The total number of reads left after filtering
tot_alignments = len(df_clone_seq)

# The number of unique references in the alignment data across all your reads
num_refs = df_clone_seq['RNAME'].nunique()

# A dictionary mapping unique reference IDs to the number of reads aligned to that reference
reads_per_ref = df_clone_seq['RNAME'].value_counts().to_dict()

# The average mapping quality for all reads left after filtering
average_quality = df_clone_seq['MAPQ'].mean()

# Print or use the variables as needed
print("Total Alignments:", tot_alignments)
print("Number of Unique References:", num_refs)
print("Reads per Reference:", reads_per_ref)
print("Average Mapping Quality:", average_quality)


# In[12]:


def pos_nucl_count(seq, pos):
    # Initialize a dictionary to store nucleotide counts at each position
    pos_counts = {}

    # Iterate through each position in the sequence
    for i, nucleotide in enumerate(seq):
        # Calculate the absolute position based on the leftmost aligning position (1-based)
        abs_pos = pos + i

        # Initialize a dictionary for the current position if not already present
        pos_counts.setdefault(abs_pos, {"A": 0, "T": 0, "C": 0, "G": 0})

        # Increment the count for the current nucleotide at the current position
        pos_counts[abs_pos][nucleotide] += 1

    return pos_counts

# Example usage
seq = "CCGA"
pos = 4

result = pos_nucl_count(seq, pos)
print(result)


# In[17]:


import pandas as pd

def pos_nucl_count_all(df, qual_thres=30, qual_base=33, nucl_all=["A", "T", "C", "G"]):
    # Initialize a dictionary to store nucleotide counts at each position
    pos2count = {}

    # Iterate through each row in the DataFrame
    for index, row in df.iterrows():
        seq = row['SEQ']
        pos = row['POS']
        qual_string = row['QUAL']

        # Iterate through each character in the quality string
        for char in qual_string:
            # Convert ASCII to integer
            qual = ord(char) - qual_base

            # Quality control: Skip low-quality positions
            if qual >= qual_thres:
                # Iterate through each position in the sequence
                for i, nucleotide in enumerate(seq):
                    # Calculate the absolute position based on the leftmost aligning position (1-based)
                    abs_pos = pos + i

                    # Initialize a dictionary for the current position if not already present
                    pos2count.setdefault(abs_pos, {nucl: 0 for nucl in nucl_all})

                    # Check if the nucleotide is in the set before updating the count
                    if nucleotide in nucl_all:
                        # Increment the count for the current nucleotide at the current position
                        pos2count[abs_pos][nucleotide] += 1

    return pos2count

# Example usage (assuming df_clone_seq is your DataFrame)
result = pos_nucl_count_all(df_clone_seq)
print(result)


# In[18]:


import pandas as pd

# Assuming you have df_clone_seq and pos_nucl_count_all function defined

# Example usage
result = pos_nucl_count_all(df_clone_seq)

# Convert the result dictionary to a DataFrame for easier handling
result_df = pd.DataFrame(result)

# Specify the output CSV file name
output_csv_file = "pos_nucl_count_result.csv"

# Write the DataFrame to a CSV file
result_df.to_csv(output_csv_file, index=True)

print(f"Result has been written to {output_csv_file}")

# Specify the output TXT file name
output_txt_file = "pos_nucl_count_result.txt"

# Write the result dictionary to a TXT file
with open(output_txt_file, 'w') as txt_file:
    for position, counts in result.items():
        txt_file.write(f"Position {position}:\n")
        for nucleotide, count in counts.items():
            txt_file.write(f"  {nucleotide}: {count}\n")

print(f"Result has been written to {output_txt_file}")


# In[19]:


import pandas as pd

# Assuming you have df_clone_seq and pos_nucl_count_all function defined

# Quality control threshold
qual_thres = 30

# Initialize an empty list to store dictionaries
summary_dicts = []

# Iterate through unique genes in the DataFrame
for gene_id, gene_df in df_clone_seq.groupby('RNAME'):
    # Get nucleotide counts for the gene
    gene_counts = pos_nucl_count_all(gene_df, qual_thres=qual_thres)

    # Iterate through positions and counts
    for pos, counts in gene_counts.items():
        summary_dict = {
            'pos': pos,
            'A': counts.get('A', 0),
            'T': counts.get('T', 0),
            'C': counts.get('C', 0),
            'G': counts.get('G', 0),
            'gene': gene_id
        }
        summary_dicts.append(summary_dict)

# Create the df_summary DataFrame from the list of dictionaries
df_summary = pd.DataFrame(summary_dicts)

# Reorder columns
df_summary = df_summary[['pos', 'A', 'T', 'C', 'G', 'gene']]

# Display the df_summary DataFrame
print(df_summary)


# In[20]:


# Specify the path to Attempted_Muts.txt
attempted_muts_path = "C:\\Users\\Ngugi\\Downloads\\Attempted_Muts.txt"

# Read Attempted_Muts.txt and extract gene mutation IDs
with open(attempted_muts_path, 'r') as file:
    mut_list = [line.strip() for line in file]

# Create a dictionary to store expected frequencies
expected_frequency = {}

# Calculate expected frequency for each gene
for mutation_id in mut_list:
    # Extract gene ID from the mutation ID
    gene_id = mutation_id.split('_')[0]

    # Increment the count for the gene in the dictionary
    expected_frequency[gene_id] = expected_frequency.get(gene_id, 0) + 1

# Normalize the counts to get the expected frequencies
total_mutations = len(mut_list)
for gene_id in expected_frequency:
    expected_frequency[gene_id] /= total_mutations

# Display the mutation list and expected frequencies
print("Mutation List:")
print(mut_list)

print("\nExpected Frequencies:")
print(expected_frequency)


# In[25]:


import pandas as pd

def extract_position(pos_part):
    return int(''.join([c for c in pos_part if c.isdigit()])) if pos_part.isdigit() else None

def generate_mut_summary(df_summary, mut_list, expected_frequency):
    mut_summary_dicts = []

    for attempted_mut in mut_list:
        components = attempted_mut.split('_')

        if len(components) < 3:
            print(f"Skipping valid mutation ID: {attempted_mut}")
            continue

        gene = components[0]
        pos_part = ''.join(filter(str.isdigit, components[1]))
        alt = components[2][1:]

        pos = extract_position(pos_part)

        if pos is None:
            print(f"Skipping invalid mutation ID: {attempted_mut}")
            continue

        undesired_mut = ""
        is_detected = False
        is_clean = True
        obs_freq = 0.0
        exp_freq = expected_frequency.get(gene, 0.0)

        if not df_summary[(df_summary['gene'] == gene) & (df_summary['pos'] == pos)].empty:
            obs_freq = df_summary.loc[(df_summary['gene'] == gene) & (df_summary['pos'] == pos), alt].values[0] / 100.0

            undesired_mutations = df_summary.loc[
                (df_summary['gene'] == gene) & (df_summary['pos'] == pos) &
                (df_summary['A'] + df_summary['T'] + df_summary['C'] + df_summary['G'] > 1)
            ]

            if not undesired_mutations.empty:
                is_clean = False
                undesired_mut = ";".join(undesired_mutations.columns[1:])

            is_detected = alt in undesired_mutations.columns

        mut_summary_dict = {
            'attempt_mut': attempted_mut,
            'gene': gene,
            'pos': pos,
            'undesired_mut': undesired_mut,
            'is_detected': is_detected,
            'is_clean': is_clean,
            'obs_freq': obs_freq,
            'exp_freq': exp_freq
        }

        mut_summary_dicts.append(mut_summary_dict)

    mut_summary = pd.DataFrame(mut_summary_dicts)

    # Display the mut_summary DataFrame
    print(mut_summary)

# Example usage assuming df_summary, mut_list, and expected_frequency are defined
generate_mut_summary(df_summary, mut_list, expected_frequency)


# In[ ]:




