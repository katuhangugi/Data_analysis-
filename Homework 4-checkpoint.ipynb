{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de4616e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "def read_data(C_Users_This_PC_Desktop_neg, C_Users_This_PC_Desktop_pos):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for path, label in [(C_Users_This_PC_Desktop_pos, 1), (C_Users_This_PC_Desktop_neg, 0)]:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split()\n",
    "                text = ' '.join(tokens)\n",
    "                train_texts.append(text)\n",
    "                train_labels.append(label)\n",
    "    \n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    \n",
    "    for path, label in [(C_Users_This_PC_Desktop_pos, 1), (C_Users_This_PC_Desktop_neg, 0)]:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split()\n",
    "                text = ' '.join(tokens)\n",
    "                test_texts.append(text)\n",
    "                test_labels.append(label)\n",
    "    \n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "    test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "    \n",
    "    return train_encodings['input_ids'], train_labels, test_encodings['input_ids'], test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de3a5807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough samples for train and test split.\n"
     ]
    }
   ],
   "source": [
    "def read_data(C_Users_This_PC_Desktop_txt_sentoken):\n",
    "    # Implementation to read data from file or any other source\n",
    "    # Replace this implementation with the appropriate code for your use case\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    # Read data from file_path and populate train_texts and train_labels\n",
    "    return train_texts, train_labels\n",
    "\n",
    "# Update the code with the appropriate file path or data source\n",
    "train_texts, train_labels = read_data('C:/Users/This_PC/Desktop/txt_sentoken')\n",
    "\n",
    "# Convert train_texts and train_labels to lists, if they are not already\n",
    "train_texts = list(train_texts)\n",
    "train_labels = list(train_labels)\n",
    "\n",
    "# Check if train_texts has enough samples for train and test split\n",
    "if len(train_texts) >= 99:\n",
    "    C_Users_This_PC_Desktop_txt_sentoken_train, C_Users_This_PC_Desktop_txt_sentoken_dev = train_test_split(\n",
    "        train_texts, train_labels, test_size=0.2, stratify=train_labels, random_state=42\n",
    "    )\n",
    "\n",
    "    dev_pos_texts = [text for text, label in zip(C_Users_This_PC_Desktop_txt_sentoken_train) if label == 1][:100]\n",
    "    dev_neg_texts = [text for text, label in zip(C_Users_This_PC_Desktop_txt_sentoken_train) if label == 0][:100]\n",
    "    dev_texts = dev_pos_texts + dev_neg_texts\n",
    "    dev_labels = [1] * 100 + [0] * 100\n",
    "else:\n",
    "    print(\"Not enough samples for train and test split.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1febfbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d637de4b0e495684f3e452fb1acda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\This PC\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\This PC\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f6b69d59b74cce9faec2e004b6ed7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b55e43d2d64201acbf39f378b22919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive texts: 1000\n",
      "Number of negative texts: 1000\n",
      "Total number of texts: 2000\n",
      "Total number of encodings: 3\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import os\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define folder paths\n",
    "test_folder_path = 'C://Users/This PC/Desktop/txt_sentoken'\n",
    "pos_folder_path = os.path.join(test_folder_path, 'pos')\n",
    "neg_folder_path = os.path.join(test_folder_path, 'neg')\n",
    "\n",
    "pos_texts = []\n",
    "for filename in os.listdir(pos_folder_path):\n",
    "    with open(os.path.join(pos_folder_path, filename), 'r') as pos_file:\n",
    "        pos_texts.append(pos_file.read())\n",
    "\n",
    "neg_texts = []\n",
    "for filename in os.listdir(neg_folder_path):\n",
    "    with open(os.path.join(neg_folder_path, filename), 'r') as neg_file:\n",
    "        neg_texts.append(neg_file.read())\n",
    "\n",
    "test_texts = pos_texts + neg_texts\n",
    "\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "print(\"Number of positive texts:\", len(pos_texts))\n",
    "print(\"Number of negative texts:\", len(neg_texts))\n",
    "print(\"Total number of texts:\", len(test_texts))\n",
    "print(\"Total number of encodings:\", len(test_encodings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ae948eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(test_dataset):\n",
    "    # Assuming you have a trained model and a way to perform inference on test data\n",
    "\n",
    "    # Perform inference on test dataset to get predicted labels and calculate test loss\n",
    "    y_pred = []  # Placeholder for predicted labels\n",
    "    test_loss = 0  # Placeholder for test loss\n",
    "    for batch in test_dataset:\n",
    "        # Assuming batch is a dictionary with keys 'input' and 'labels' representing input data and labels\n",
    "        input_data = batch['input']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        # Perform inference on input data to get predicted labels\n",
    "        # Assuming model is a trained model and predict() is a method that performs inference and returns predicted labels\n",
    "        batch_pred = model.predict(input_data)\n",
    "        \n",
    "        # Accumulate predicted labels\n",
    "        y_pred.extend(batch_pred.tolist())\n",
    "        \n",
    "        # Calculate batch loss and accumulate test loss\n",
    "        # Assuming loss_func is a loss function that takes in input data, predicted labels, and true labels\n",
    "        batch_loss = loss_func(input_data, batch_pred, labels)\n",
    "        test_loss += batch_loss.item()  # Assuming batch_loss is a scalar tensor, convert to float and accumulate\n",
    "    \n",
    "    return test_loss, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1945bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_dataset(file_path):\n",
    "    # Load your test dataset from file_path and preprocess it as necessary\n",
    "    # Return the processed test dataset\n",
    "    # Example implementation:\n",
    "    # Load the dataset from file path\n",
    "    # Assuming your dataset is stored in a CSV file with 'input' and 'labels' columns\n",
    "    import pandas as pd\n",
    "    test_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Preprocess the input data and labels\n",
    "    input_data = preprocess_input(test_df['input'])\n",
    "    labels = preprocess_labels(test_df['labels'])\n",
    "\n",
    "    # Create a dataset using the input data and labels\n",
    "    test_dataset = create_dataset(input_data, labels)\n",
    "\n",
    "    return test_dataset\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = load_test_dataset('C:\\\\Users\\\\This PC\\\\Desktop\\\\txt_sentoken')  # Replace with your own function or code to load test dataset\n",
    "\n",
    "# Evaluate model on test dataset\n",
    "test_loss, y_pred = model_evaluation(test_dataset)\n",
    "\n",
    "# Calculate performance metrics\n",
    "y_true = []  # Populate with true labels from test_dataset or obtain from your data\n",
    "for batch in test_dataset:\n",
    "    labels = batch['labels']\n",
    "    y_true.extend(labels.tolist())\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, pos_label='pos')\n",
    "recall = recall_score(y_true, y_pred, pos_label='pos')\n",
    "f1 = f1_score(y_true, y_pred, pos_label='pos')\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
    "print(f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c579b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bea67a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
