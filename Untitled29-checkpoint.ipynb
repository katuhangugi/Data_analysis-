{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e154bac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\This PC\\anaconda3\\lib\\site-packages\\pyspark\\sql\\context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'lag_prc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15088\\3187356950.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Filter the data to include only lag(prc) greater than $5, hexcd (1,2,3), sharecode (10,11) and lag market cap not in the bottom 20%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mfilteredDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignalDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignalDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlag_prc\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msignalDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhexc\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msignalDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msharecode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msignalDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlag_mktcap_pct\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Compute the equal-weighted and value-weighted portfolios re-balancing on a monthly basis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1986\u001b[0m         \"\"\"\n\u001b[0;32m   1987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1988\u001b[1;33m             raise AttributeError(\n\u001b[0m\u001b[0;32m   1989\u001b[0m                 \u001b[1;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1990\u001b[0m             )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'lag_prc'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd\n",
    "\n",
    "# Create a SparkContext and an SQLContext to read the signal parquet file\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Read the signal parquet file\n",
    "signalDF = sqlContext.read.parquet(\"C:\\\\Users\\\\This PC\\\\Desktop\\\\bnbm\\\\hw2_signal.parquet\")\n",
    "\n",
    "# Filter the data to include only lag(prc) greater than $5, hexcd (1,2,3), sharecode (10,11) and lag market cap not in the bottom 20%\n",
    "filteredDF = signalDF.filter((signalDF.lag_prc > 5) & (signalDF.hexc in (1,2,3)) & (signalDF.sharecode in (10,11)) & (signalDF.lag_mktcap_pct > 0.2))\n",
    "\n",
    "# Compute the equal-weighted and value-weighted portfolios re-balancing on a monthly basis\n",
    "equalWeightedPortfolio = filteredDF.groupBy(\"date\").agg({\"return\": \"avg\"}).withColumnRenamed(\"avg(return)\", \"equal_weighted_return\")\n",
    "valueWeightedPortfolio = filteredDF.groupBy(\"date\").agg({\"lag_mktcap\": \"avg\", \"return\": \"avg\"}).withColumnRenamed(\"avg(lag_mktcap)\", \"value_weighted_mktcap\").withColumnRenamed(\"avg(return)\", \"value_weighted_return\")\n",
    "valueWeightedPortfolio = valueWeightedPortfolio.withColumn(\"value_weighted_return\", valueWeightedPortfolio[\"value_weighted_return\"]/valueWeightedPortfolio[\"value_weighted_mktcap\"])\n",
    "\n",
    "# Does the equal or value-weighted strategy have a higher average return?\n",
    "equalWeightedAverageReturn = equalWeightedPortfolio.agg({\"equal_weighted_return\": \"avg\"}).collect()[0][0]\n",
    "valueWeightedAverageReturn = valueWeightedPortfolio.agg({\"value_weighted_return\": \"avg\"}).collect()[0][0]\n",
    "\n",
    "if equalWeightedAverageReturn > valueWeightedAverageReturn:\n",
    "    print(\"The equal-weighted strategy has a higher average return\")\n",
    "else:\n",
    "    print(\"The value-weighted strategy has a higher average return\")\n",
    "\n",
    "# Present the bar-plot of equal-weighted returns and value-weighted returns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "equalWeightedReturns = equalWeightedPortfolio.collect()\n",
    "valueWeightedReturns = valueWeightedPortfolio.collect()\n",
    "\n",
    "equalWeightedDates = [x[0] for x in equalWeightedReturns]\n",
    "equalWeightedReturns = [x[1] for x in equalWeightedReturns]\n",
    "valueWeightedDates = [x[0] for x in valueWeightedReturns]\n",
    "valueWeightedReturns = [x[2] for x in valueWeightedReturns]\n",
    "\n",
    "plt.bar(equalWeightedDates, equalWeightedReturns, width=0.5, label=\"Equal-Weighted Returns\")\n",
    "plt.bar(valueWeightedDates, valueWeightedReturns, width=0.5, label=\"Value-Weighted Returns\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# P&L curve\n",
    "equalWeightedCumReturns = [sum(equalWeightedReturns[0:i+1]) for i in range(len(equalWeightedReturns))]\n",
    "valueWeightedCumReturns = [sum(valueWeightedReturns[0:i+1]) for i in range(len(valueWeightedReturns))]\n",
    "\n",
    "plt.plot(equalWeightedDates, equalWeightedCumReturns, label=\"Equal-Weighted Cumulative Returns\")\n",
    "plt.plot(valueWeightedDates, valueWeightedCumReturns, label=\"Value-Weighted Cumulative Returns\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Present the factor loadings to the four factor model and five factor model.\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Vectorize the feature columns\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"lag_prc_pct_change\", \"lag_mktcap_pct_change\", \"lag_beta\", \"lag_divyield\", \"mom\"], outputCol=\"features\")\n",
    "vectorizedDF = vectorAssembler.transform(filteredDF)\n",
    "\n",
    "# Create a LinearRegression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"return\", predictionCol=\"prediction\")\n",
    "\n",
    "# Fit the model to the data\n",
    "lrModel = lr.fit(vectorizedDF)\n",
    "\n",
    "# Print the factor loadings\n",
    "print(\"Intercept:\", lrModel.intercept)\n",
    "print(\"Coefficients:\", lrModel.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe1e23cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4704\\2026702627.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Read file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\This PC\\\\Desktop\\\\bnbm\\\\hw2_signal.parquet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Filter data according to lag(prc) greater than $5, hexcd (1,2,3), sharecode (10,11) and lag market cap not in the bottom 20% that month\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m     \"\"\"\n\u001b[1;32m--> 491\u001b[1;33m     \u001b[0mimpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     return impl.read(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mget_engine\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0merror_msgs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"\\n - \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         raise ImportError(\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[1;34m\"Unable to find a usable engine; \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;34m\"tried using: 'pyarrow', 'fastparquet'.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read file\n",
    "data = pd.read_parquet(\"C:\\\\Users\\\\This PC\\\\Desktop\\\\bnbm\\\\hw2_signal.parquet\")\n",
    "\n",
    "# Filter data according to lag(prc) greater than $5, hexcd (1,2,3), sharecode (10,11) and lag market cap not in the bottom 20% that month\n",
    "filtered_data = data[(data['lag_prc'] > 5) & ((data['hexcds'] == 1) | (data['hexcds'] == 2) | (data['hexcds'] == 3)) & ((data['sharecode'] == 10) | (data['sharecode'] == 11)) & (data['lag_mkvalt'] > data['lag_mkvalt'].quantile(0.2))]\n",
    "\n",
    "# Compute the equal-weighted and value-weighted portfolios re-balancing on a monthly basis\n",
    "# Equal-Weighted Portfolio\n",
    "ew_port = filtered_data.groupby(['date']).apply(lambda x: x/x.sum())\n",
    "ew_port['ew_port_ret'] = ew_port.groupby(['date'])['ret'].transform('mean')\n",
    "\n",
    "#Value-Weighted Portfolio\n",
    "vw_port = filtered_data.groupby(['date']).apply(lambda x: x*x['mkvalt']/x['mkvalt'].sum())\n",
    "vw_port['vw_port_ret'] = vw_port.groupby(['date'])['ret'].transform('mean')\n",
    "\n",
    "# Does the equal or value-weighted strategy have a higher average return?\n",
    "print(f'Equal-Weighted Portfolio Average Return: {ew_port[\"ew_port_ret\"].mean():.2f}')\n",
    "print(f'Value-Weighted Portfolio Average Return: {vw_port[\"vw_port_ret\"].mean():.2f}')\n",
    "\n",
    "# Is it common for the equal or value-weighted strategies to have higher returns?\n",
    "print('It is common for the value-weighted strategies to have higher returns.')\n",
    "\n",
    "# Present the bar-plot of equal-weighted returns and value-weighted returns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar([\"Equal-Weighted\",\"Value-Weighted\"], [ew_port[\"ew_port_ret\"].mean(), vw_port[\"vw_port_ret\"].mean()])\n",
    "plt.title(\"Average Returns\")\n",
    "plt.show()\n",
    "\n",
    "# P&L curve \n",
    "plt.plot(ew_port.groupby(['date'])['ew_port_ret'].cumsum(), label=\"Equal-Weighted\")\n",
    "plt.plot(vw_port.groupby(['date'])['vw_port_ret'].cumsum(), label=\"Value-Weighted\")\n",
    "plt.title(\"Cumulative Returns\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ffdc37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
